# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

#The GCP Project ID
PROJECT_ID=$(gcloud config list project --format "value(core.project)")
#This will become the name of the Service account and the name of the KEY json file
KEY_NAME=dflow-accounts
#Name of the key file
KEY_FILE=${KEY_NAME}.json
#Place to write the Key for the Service account
KEY_DIR=/home/sufyaankazi/keys/
#The full prefix for the service account including key name and project name
SERVICE_ACC=$KEY_NAME@$PROJECT_ID
#The roles required for the service account
# - Storage roles are required for basic read/write + additoinal priveleges needed by dataflow to write and cleanup temporary objects
# - BigQuery roles needed to write data into BigQuery
# - Dataflow worker role needed to act as a 'controller' for jobs and admin so that compute instances can be launched and jobs created
SERVICE_ACC_ROLES="roles/storage.admin roles/storage.objectAdmin roles/storage.objectCreator roles/storage.objectViewer roles/bigquery.dataEditor roles/bigquery.jobUser roles/dataflow.worker roles/dataflow.admin"

#The list of API's that should be enabled
APIS="iam compute dataflow logging storage-component storage-api bigquery pubsub"

#The bucket where the bulk of processing will occu
BUCKET_NAME=sufbankdata
TEMPBUCKET=suftempbucket

#The region to create the BigQuery entitie sin
BQ_REGION=europe-west2
#BigQuery dtaset to create and use
DATASET=sufbankingds
#BigQuery table to create and use
TABLE=bankingtxns
#Current date/time - used as the name of temp config file for applying permissions needed on BigQuery
DATE=$(date '+%Y%m%d%H:%M:%S')

#Dataflow Region and Zone
DF_REGION=europe-west1
DF_ZONE=${DF_REGION}-c

#Mapping file to help deduce the transaciton category from the description in the test
CONFIG_FILE=banking.config
#The main java class that builds the Beam Pipeline
MAIN_CLASS=com.suf.dataflow.banking.AccountsPrePrep
